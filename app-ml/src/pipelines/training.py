import pandas as pd
import numpy as np
import joblib
import argparse
from dotenv import load_dotenv
from dataclasses import dataclass
from dataclasses import asdict

from common.monitoring import log_feature_importance_catboost

import mlflow
from mlflow.exceptions import MlflowException
from mlflow.tracking import MlflowClient
from mlflow.models import infer_signature
from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID

import optuna
import logging
from pathlib import Path
import os,sys

from typing import Dict, Tuple, Any
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import RepeatedKFold, cross_validate

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import root_mean_squared_error, make_scorer

from catboost import CatBoostRegressor

from common.utils import configure_mlflow, time_series_split
from common.data_classes import TrainResult



#project_root = Path(__file__).resolve().parent.parent.parent.parent
project_root = Path("/app")


logger = logging.getLogger(__name__)
logging.basicConfig(filename=f"{project_root}/logs/training_pipeline.log", filemode="a", encoding='utf-8',
                    level=logging.INFO,
                    format='%(asctime)s %(filename)s->%(funcName)s():%(lineno)s %(message)s ')

load_dotenv(dotenv_path=project_root / "app-ml" / "src" / "pipelines" / ".env", override=True)


class TrainingPipeline:
    """
    A pipeline class for training and optimizing machine learning models,
    specifically CatBoost, using configuration-driven parameters and Optuna for tuning.

    Attributes:
        config (Dict[str, Any]): Configuration dictionary with training parameters.
        optuna_config (Dict[str, Any]): Subset of config containing Optuna-specific settings.
        search_space (Dict[str, Any]): Hyperparameter search space for tuning.
    """

    def __init__(self, config: Dict[str, Any]) -> None:
        """
        Initialize the TrainingPipeline with the provided configuration.

        Args:
            config (Dict[str, Any]): Full pipeline configuration dictionary.
        """
        self.config: Dict[str, Any] = config['training']
        self.config_model: Dict[str, Any] = config['pipeline_runner']
        self.optuna_config: Dict[str, Any] = self.config.get('optuna', {})
        self.search_space: Dict[str, Any] = self.config['optuna']['search_space']

    @staticmethod
    def make_target(df: pd.DataFrame, target_params: Dict[str, str]) -> pd.DataFrame:
        """
        Create a shifted target column for forecasting tasks.

        Args:
            df (pd.DataFrame): Input DataFrame.
            target_params (Dict[str, str]): Parameters dict with keys:
                - 'target_column': source column name
                - 'shift_period': how far to shift the target forward
                - 'new_target_name': name of the resulting target column

        Returns:
            pd.DataFrame: DataFrame with a new target column.
        """
        shift_period = target_params['shift_period']
        df[target_params['new_target_name']] = df[target_params['target_column']].shift(-shift_period).ffill()
        return df

    def prepare_dataset(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        Prepares training and testing datasets by applying a target transformation
        and splitting by fraction (no shuffling).

        Args:
            df (pd.DataFrame): Input DataFrame with features and target.

        Returns:
            Tuple containing:
                - x_train (pd.DataFrame): Training features
                - x_test (pd.DataFrame): Testing features
                - y_train (pd.Series): Training target
                - y_test (pd.Series): Testing target
        """
        df = self.make_target(df, target_params=self.config['target_params'])
        logging.info("separating dataframe for model training and evaluation")
        # logging.info(f'dropping columns {self.config['non_training_columns']} and {self.config['target_params']['new_target_name']} to create X dataframe ')

        x, y = df.drop(self.config['non_training_columns'], axis=1), df[self.config['target_params']['new_target_name']]
        x = x.drop(self.config['target_params']['new_target_name'], axis=1)
        for col in x.columns.to_list():
            for i in self.config['non_training_columns']:
                if col == i:
                    print("non training columns not dropped!")
                    logging.info("non training columns not dropped!")

        print("using dataframe generated by prepare dataset method")

        train_size = int(self.config['train_fraction'] * len(df))
        x_train, x_test = x[:train_size], x[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        logging.info("prepared train and test data")

        return x_train, x_test, y_train, y_test

    def tune_hyperparams(
            self,
            x_train: pd.DataFrame,
            y_train: pd.Series,
            x_test: pd.DataFrame,
            y_test: pd.Series
    ) -> Tuple[Any, optuna.Study]:
        """
        Perform hyperparameter tuning using Optuna, then retrain the model
        using the best configuration on the full training data.

        Args:
            x_train (pd.DataFrame): Training features.
            y_train (pd.Series): Training targets.
            x_test (pd.DataFrame): Test features.
            y_test (pd.Series): Test targets.

        Returns:
            Tuple containing:
                - Trained CatBoost model with best parameters
                - Completed Optuna Study object
        """
        np.random.seed(42)

        def objective(trial: optuna.Trial) -> float:
            ss = self.search_space
            params = {
                "learning_rate": trial.suggest_float(
                    "learning_rate", ss["learning_rate"]["low"], ss["learning_rate"]["high"],
                    log=ss["learning_rate"].get("log", False)
                ),
                "depth": trial.suggest_int("depth", ss["depth"]["low"], ss["depth"]["high"]),
                "l2_leaf_reg": trial.suggest_float(
                    "l2_leaf_reg", ss["l2_leaf_reg"]["low"], ss["l2_leaf_reg"]["high"],
                    log=ss["l2_leaf_reg"].get("log", False)
                ),
                "iterations": self.config["iterations"],
                "loss_function": self.config["loss_function"],
                "verbose": self.config.get("verbose", 0)
            }

            # Manual time-based validation split
            train_idx = int(self.config['train_fraction'] * len(x_train))
            x_tr, x_val = x_train.iloc[:train_idx], x_train.iloc[train_idx:]
            y_tr, y_val = y_train.iloc[:train_idx], y_train.iloc[train_idx:]

            model = CatBoostRegressor(**params, random_seed=42, allow_writing_files=False)
            model.fit(
                x_tr, y_tr,
                eval_set=(x_val, y_val),
                early_stopping_rounds=self.config.get("early_stopping_rounds", 100),
                use_best_model=True,
                verbose=False

            )

            preds = model.predict(x_val)
            rmse = np.sqrt(mean_squared_error(y_val, preds))
            trial.set_user_attr("best_iteration", model.get_best_iteration())
            return rmse

        # Run Optuna study
        n_trials = self.optuna_config["n_trials"]
        study = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=42))
        logging.info("running optuna parameter optimization")
        study.optimize(objective, n_trials=n_trials)

        # Train final model on full training data with best parameters
        best_params = study.best_params.copy()
        best_params.update({
            "iterations": study.best_trial.user_attrs["best_iteration"],
            "loss_function": self.config["loss_function"],
            "verbose": False

        })

        # Concatenate training and testing data
        x_train_test = pd.concat([x_train, x_test], axis=0)
        y_train_test = pd.concat([y_train, y_test], axis=0)

        logging.info("fitting model with best params from optuna study")
        final_model = CatBoostRegressor(**best_params, random_seed=42, allow_writing_files=False)
        final_model.fit(x_train_test, y_train_test, verbose=False)

        logging.info(
            "model fitted with best params from optuna trials and complete training data used for parameter tuning")

        return final_model, study

    def run(self, df: pd.DataFrame) -> Any:
        """
        Run the full training pipeline:
        1. Generate target column
        2. Train-test split
        3. Tune hyperparameters with Optuna
        4. Train final model using best parameters

        Args:
            df (pd.DataFrame): Input training DataFrame with features and target.

        Returns:
            - Any: Trained model (e.g., CatBoostRegressor)
            - Best parameters found through Optuna
        """
        x_train, x_test, y_train, y_test = self.prepare_dataset(df)

        model, _ = self.tune_hyperparams(x_train, y_train, x_test, y_test)
        return model

        """
        Trains and evaluates the model. Logs the model to mlflow and updates its version if
        evaluation metrics is below a set threshold

        Model is evaluated using cross validation using a split train set, and if successfull,
        fitted on the complete input training set before logging in mlflow

        Lastly, the latest version of the model is transitioned to staging

        Args:
            df (pd.DataFrame) : Input training DataFrame with features and target

        Returns:
            Any: logging status that trained model was either succesfully logged in mlflow or not, due to
                 poor evaluation metrics


        """

    def evaluate_rmse_cv(self, model, X_val, y_val, n_splits: int, n_repeats: int) -> float:
        """
        Args:
            model: trained model
            X_val: validation set features
            y_val: validation set target
            n_splits: number of cross-validation folds
            n_repeats: number of repetitions

        Returns:
            mean cross validation rmse score

        """

        scoring = {"rmse": "neg_root_mean_squared_error"}
        kfold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=self.config["random_state"])
        cvs = cross_validate(model, X_val, y_val, cv=kfold, scoring=scoring, n_jobs=1)
        return float(-cvs["test_rmse"].mean())

    def train_log_model(self, df: pd.DataFrame) -> bool:

        """
        Method to evaluate a hyperparameter tuned model from the run step in this class.

        Sets up a mlflow tracking server, conducts cross evaluation based on a holdout set,
        and logs the model,metrics and params in mlflow if a threshold rmse metric is maintained.

        Before logging model on mlflow, model is fitted with the entire training dataset.

        """

        logger.info('training model with tuned params from run step in training pipeline')
        #model = self.run(df)


        logger.info("loading environment vars for mlflow logging")
        load_dotenv()

        # --- Configure MLflow (Databricks)  ---
        tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "databricks")
        registry_uri = os.getenv("MLFLOW_REGISTRY_URI", "databricks")

        experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "/Shared/penicillin time series model")

        logging.info("setting up mlflow tracking server")
        mlflow_client = configure_mlflow(tracking_uri_key=tracking_uri, registry_uri_key=registry_uri,
                                         experiment_name=experiment_name)
        logging.info("loading model from mlflow")
        model = mlflow.sklearn.load_model("models:/workspace.default.catboost_model/11")


        x_train, x_test, y_train, y_test = self.prepare_dataset(df)
        cross_validation_data = time_series_split(self.config['train_fraction'], x_train, y_train)

        logging.info("fitting model with subset of training data for cross validation")
        model.fit(cross_validation_data.X_train, cross_validation_data.y_train, plot=False)

        scoring = {"rmse": "neg_root_mean_squared_error"}

        kfold = RepeatedKFold(n_splits=6, n_repeats=3, random_state=self.config['random_state'])
        logging.info("performing cross validation of model")
        cvs = cross_validate(model, cross_validation_data.X_val,
                             cross_validation_data.y_val,
                             cv=kfold, scoring=scoring,
                             n_jobs=1)
        # insert stratified cross validate func and log the mean scores.

        score_rmse = -cvs["test_rmse"].mean()
        logging.info(f"cross validated rmse score was {score_rmse}")

        threshold = self.config['cv_threshold']

        if score_rmse < threshold:
            logging.info(f'cv rmse score is below {threshold}, proceeding with logging model in mlflow')
            x_train_test = pd.concat([x_train, x_test], axis=0)
            y_train_test = pd.concat([y_train, y_test], axis=0)

            logging.info('fitting model with total production data')
            model.fit(x_train_test, y_train_test)

            logging.info("saving model locally")
            #joblib.dump(model, project_root / "models" / "trained_catboost_model.pkl")
            model.save_model(project_root / "models"/"prod"/"trained_catboost_model.cbm")

            logging.info("extracting feature importances and logging to mlflow")
            log_feature_importance_catboost(model
                                                , x_train_test
                                                , project_root=project_root
                                                , artifact_dir="feature_importances")

            with mlflow.start_run(run_name="catboost_model_training") as run:

                logging.info("logging cross validation metrics and params")
                mlflow.log_metric('test_cv_rmse', score_rmse)
                mlflow.log_params(model.get_params())

                dataset_train = mlflow.data.from_pandas(x_train_test, name="training_df")

                logging.info("logging training dataset")
                mlflow.log_input(dataset_train, context="training")

                from datetime import datetime
                ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
                dataset_uri = f"gs://pennicilin_batch_yield/training_data/version={ts}/train.parquet"
                logging.info("saving training dataset in gcs")
                df.to_parquet(dataset_uri)
                logging.info("logging training dataset uri")
                mlflow.log_param("training_data_uri", dataset_uri)

                signature = infer_signature(x_test, model.predict(x_test))

                logging.info("logging model in mlflow")
                model_info = mlflow.sklearn.log_model(
                    sk_model=model,
                    artifact_path="model",
                    input_example=x_train[:10],
                    registered_model_name=self.config['mlflow_name']
                )
                logging.info("model trained and logged in mlflow")
        else:
            logging.info(f"new model did not clear threshold with validation test rmse score being {threshold}")

        training_metrics = TrainResult(rmse_cv=score_rmse,
                                       training_data_uri=dataset_uri,
                                       model_name=self.config['mlflow_name'],
                                       run_id=run.info.run_id,
                                       run_name=run.data.tags.get("mlflow.runName"),
                                       num_features_training=x_train.shape[1],
                                       target_variable=self.config["target_params"]["new_target_name"])

       # metrics_json = asdict(training_metrics)
        #logging.info("logging training metrics as json in mlflow")
        #try:
         #   mlflow.log_dict(metrics_json, "training_metrics.json")
        #e#xcept MlflowException:
         #   logging.info("error in logging training metrics")
        return True

        #return training_metrics

# class works with updated use case

